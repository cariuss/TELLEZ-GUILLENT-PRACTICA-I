{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207adb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps | torch: 2.9.0 | mac: arm64\n"
     ]
    }
   ],
   "source": [
    "import torch, random, numpy as np, platform\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Device:\", device, \"| torch:\", torch.__version__, \"| mac:\", platform.machine())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68dac33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases: ['Alianzas', 'Innovacion', 'Macroeconomia', 'Otra', 'Regulaciones', 'Reputacion', 'Sostenibilidad'] | NUM_LABELS: 7\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import pandas as pd\n",
    "\n",
    "df = kagglehub.dataset_load(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"kevinmorgado/spanish-news-classification\",\n",
    "    \"df_total.csv\",\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df.rename(columns={\"news\": \"text\", \"Type\": \"label\"})[[\"text\", \"label\"]]\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"label\"])\n",
    "NUM_LABELS = len(le.classes_)\n",
    "print(\"Clases:\", list(le.classes_), \"| NUM_LABELS:\", NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614db9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cariuss/Desktop/TELLEZ-GUILLENT-PRACTICA-I /.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457cb941af4040f88cd4ed2dfd2c8752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/973 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3def03afca431d84890671495d8eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25abe1b75e4b4313ba65ccdcd23517bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-multilingual-cased\"  # rápido y suficiente\n",
    "MAX_LENGTH = 32\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "hf = Dataset.from_pandas(df)\n",
    "splits = hf.train_test_split(test_size=0.2, seed=SEED)\n",
    "tmp = splits[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "train_ds, val_ds, test_ds = splits[\"train\"], tmp[\"train\"], tmp[\"test\"]\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tok(\n",
    "        batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "val_ds = val_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "test_ds = test_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95def1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "# Congela el encoder (entrenas solo la cabeza => mucho más rápido)\n",
    "for p in model.base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds, average=\"weighted\"),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tok, padding=\"longest\")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/tmp/bert-trash\",            # ✅ carpeta temporal del sistema (se borra sola)\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=\"no\",               # ✅ evita checkpoints de evaluación\n",
    "    save_strategy=\"no\",                     # ✅ no guarda checkpoints\n",
    "    num_train_epochs=2,                     # prueba rápida\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"no\",                  # ✅ no crea logs\n",
    "    report_to=\"none\",                       # ✅ no usa TensorBoard / WandB\n",
    "    disable_tqdm=False,                     # mantiene barra de progreso visible\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tok,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea82325",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(test_ds)\n",
    "print({k: round(v, 4) for k, v in metrics.items() if isinstance(v, (int, float))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80553ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 📊 RESULTADOS FINALES ======================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "# --- Predicciones sobre el conjunto de prueba ---\n",
    "pred = trainer.predict(test_ds)\n",
    "y_true = pred.label_ids\n",
    "y_pred = np.argmax(pred.predictions, axis=-1)\n",
    "\n",
    "# --- Métricas globales ---\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"weighted\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"=== MÉTRICAS GLOBALES ===\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "# --- Reporte por clase ---\n",
    "class_names = list(le.classes_)\n",
    "print(\"\\n=== REPORTE POR CLASE ===\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
    "\n",
    "\n",
    "# --- F1 por clase ---\n",
    "prec_c, rec_c, f1_c, sup_c = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=None, zero_division=0\n",
    ")\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(class_names, f1_c, color=\"#007acc\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"Desempeño por clase (F1)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Precisión y Recall por clase ---\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.35\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(x - width / 2, prec_c, width, label=\"Precisión\", color=\"#009e73\")\n",
    "plt.bar(x + width / 2, rec_c, width, label=\"Recall\", color=\"#d55e00\")\n",
    "plt.xticks(x, class_names, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.title(\"Precisión y Recall por clase\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tellez-guillent-practica-i-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
