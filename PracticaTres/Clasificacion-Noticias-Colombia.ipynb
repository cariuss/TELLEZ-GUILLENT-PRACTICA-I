{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec19e8f4-ac60-49b0-b98e-966bca2f675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9233c007-f1ff-487c-9d9f-74b2cdccb9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>news</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3201905</td>\n",
       "      <td>Durante el foro La banca articulador empresari...</td>\n",
       "      <td>Otra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3210288</td>\n",
       "      <td>El regulador de valores de China dijo el domin...</td>\n",
       "      <td>Regulaciones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3240676</td>\n",
       "      <td>En una industria históricamente masculina como...</td>\n",
       "      <td>Alianzas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3342889</td>\n",
       "      <td>Con el dato de marzo el IPC interanual encaden...</td>\n",
       "      <td>Macroeconomia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.larepublica.co/redirect/post/3427208</td>\n",
       "      <td>Ayer en Cartagena se dio inicio a la versión n...</td>\n",
       "      <td>Otra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url  \\\n",
       "0  https://www.larepublica.co/redirect/post/3201905   \n",
       "1  https://www.larepublica.co/redirect/post/3210288   \n",
       "2  https://www.larepublica.co/redirect/post/3240676   \n",
       "3  https://www.larepublica.co/redirect/post/3342889   \n",
       "4  https://www.larepublica.co/redirect/post/3427208   \n",
       "\n",
       "                                                news           Type  \n",
       "0  Durante el foro La banca articulador empresari...           Otra  \n",
       "1  El regulador de valores de China dijo el domin...   Regulaciones  \n",
       "2  En una industria históricamente masculina como...       Alianzas  \n",
       "3  Con el dato de marzo el IPC interanual encaden...  Macroeconomia  \n",
       "4  Ayer en Cartagena se dio inicio a la versión n...           Otra  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the path to the file you'd like to load\n",
    "file_path = \"df_total.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.dataset_load(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"kevinmorgado/spanish-news-classification\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e9d026-91c6-4cce-be84-c98a246d680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Preprocesamiento: Renombrar y Mapear Etiquetas ---\n",
      "Dataset final con 1217 muestras.\n",
      "Clases detectadas: ['Alianzas', 'Innovacion', 'Macroeconomia', 'Otra', 'Regulaciones', 'Reputacion', 'Sostenibilidad'] (7 clases)\n",
      "Mapeo (Etiqueta Original -> Entero): {'Alianzas': 0, 'Innovacion': 1, 'Macroeconomia': 2, 'Otra': 3, 'Regulaciones': 4, 'Reputacion': 5, 'Sostenibilidad': 6}\n"
     ]
    }
   ],
   "source": [
    "# --- PREPARACIÓN DEL DATASET PARA BERT ---\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"--- 1. Preprocesamiento: Renombrar y Mapear Etiquetas ---\")\n",
    "\n",
    "# 1. Renombrar columnas: 'news' (contenido) -> 'text', 'Type' (clasificación) -> 'label'\n",
    "df.rename(columns={'news': 'text', 'Type': 'label'}, inplace=True)\n",
    "\n",
    "# Limpiar valores nulos y resetear el índice\n",
    "df = df[['text', 'label']].dropna().reset_index(drop=True)\n",
    "\n",
    "# 2. Mapeo de categorías a enteros (Necesario para el modelo de clasificación)\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "\n",
    "categories = le.classes_.tolist()\n",
    "NUM_LABELS = len(categories)\n",
    "\n",
    "# Crear el mapeo de categorías para referencia futura (opcional)\n",
    "label_map = dict(zip(categories, range(NUM_LABELS)))\n",
    "\n",
    "print(f\"Dataset final con {len(df)} muestras.\")\n",
    "print(f\"Clases detectadas: {categories} ({NUM_LABELS} clases)\")\n",
    "print(f\"Mapeo (Etiqueta Original -> Entero): {label_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ee928b-b476-4824-a15a-a89ee456dcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- División de Conjuntos (Train, Validation, Test) ---\n",
      "Tamaño de Entrenamiento: 973\n",
      "Tamaño de Validación: 122\n",
      "Tamaño de Prueba: 122\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Conversión y División de Conjuntos ---\n",
    "\n",
    "print(\"--- División de Conjuntos (Train, Validation, Test) ---\")\n",
    "\n",
    "# Conversión a formato Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Definir las proporciones\n",
    "test_size_ratio = 0.2  # 20% para Test+Validation\n",
    "valid_size_ratio = 0.5 # 50% de ese 20% (es decir, 10% del total)\n",
    "\n",
    "hf_dataset_splits = hf_dataset.train_test_split(test_size=test_size_ratio, seed=42)\n",
    "# Dividir el conjunto de prueba/validación\n",
    "test_valid_split = hf_dataset_splits['test'].train_test_split(test_size=valid_size_ratio, seed=42)\n",
    "\n",
    "train_dataset = hf_dataset_splits['train']\n",
    "validation_dataset = test_valid_split['train']\n",
    "test_dataset = test_valid_split['test']\n",
    "\n",
    "print(f\"Tamaño de Entrenamiento: {len(train_dataset)}\")\n",
    "print(f\"Tamaño de Validación: {len(validation_dataset)}\")\n",
    "print(f\"Tamaño de Prueba: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4073759-27dc-4553-ac28-c4603265dc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tokenización de Datos para BERT ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f7728b67ff4401b33242f31940ab6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/973 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51ebc34d0984e1f8d06e03185d47afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9499547197407bb6027fe9bf982e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización completa. Los datos están listos para ser convertidos a tensores de TensorFlow.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Tokenización (Paso 2 del desarrollo) ---\n",
    "\n",
    "from transformers import AutoTokenizer, logging\n",
    "\n",
    "# Suprimir advertencias de Hugging Face para un entorno más limpio\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "print(\"--- Tokenización de Datos para BERT ---\")\n",
    "\n",
    "# Definiciones de configuración\n",
    "MODEL_NAME = 'dccuchile/bert-base-spanish-wwm-cased' # BERT pre-entrenado en español\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "# NUM_LABELS ya está definida en una celda anterior\n",
    "\n",
    "# Cargar Tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Función de Tokenización:\n",
    "def tokenize_function(examples):\n",
    "    # Truncation y padding son cruciales para uniformar la entrada de BERT\n",
    "    return tokenizer(examples[\"text\"], \n",
    "                     truncation=True, \n",
    "                     padding='max_length', \n",
    "                     max_length=MAX_LENGTH)\n",
    "\n",
    "# Aplicar tokenización a los conjuntos\n",
    "# CORRECCIÓN: Quitamos '__index_level_0__' de remove_columns, ya que no existe.\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "tokenized_validation = validation_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "print(\"Tokenización completa. Los datos están listos para ser convertidos a tensores de TensorFlow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f116759d-4d45-4139-bec2-ca2fa03f7ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Carga del Modelo Pre-entrenado (Transfer Learning) y Compilación ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jtellez/Library/Caches/pypoetry/virtualenvs/tellez-guillent-practica-i-WaD7jEWf-py3.13/lib/python3.13/site-packages/datasets/arrow_dataset.py:405: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo BERT compilado. ¡Listo para el Fine-Tuning!\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Carga del Modelo, Conversión a TensorFlow Dataset y Compilación ---\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf \n",
    "\n",
    "# RE-DEFINICIÓN DE VARIABLES CLAVE (Asegura que estén disponibles)\n",
    "MODEL_NAME = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5 # Tasa de aprendizaje óptima para fine-tuning\n",
    "EPOCHS = 3 \n",
    "# Nota: NUM_LABELS debe haber sido definido en la Celda 3\n",
    "\n",
    "print(\"--- Carga del Modelo Pre-entrenado (Transfer Learning) y Compilación ---\")\n",
    "\n",
    "# Cargar el modelo BERT para la tarea de Clasificación de Secuencias\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "# Convertir a tensores de TensorFlow (tf.data.Dataset)\n",
    "# Este paso es crucial para optimizar el entrenamiento en Keras/TensorFlow\n",
    "tf_train = tokenized_train.to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask'],\n",
    "    label_cols=['label'],\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "tf_validation = tokenized_validation.to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask'],\n",
    "    label_cols=['label'],\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Compilar el modelo (Ajuste de hiperparámetros - Paso 3 del desarrollo)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "print(\"Modelo BERT compilado. ¡Listo para el Fine-Tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b833f26-0053-4d78-94eb-fb0b8d079bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando Fine-Tuning de BERT por 3 épocas ---\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Entrenamiento (Fine-Tuning) ---\n",
    "\n",
    "print(f\"--- Iniciando Fine-Tuning de BERT por {EPOCHS} épocas ---\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    tf_train,\n",
    "    validation_data=tf_validation,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Entrenamiento (Fine-Tuning) completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03add3-c43a-4920-93cf-1ac48fd58fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Evaluación del Rendimiento (Paso 4 del desarrollo) ---\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "print(\"--- Evaluación Final en Conjunto de Prueba ---\")\n",
    "\n",
    "# Preparar el conjunto de prueba en formato tf.data.Dataset\n",
    "tf_test = tokenized_test.to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask'],\n",
    "    label_cols=['label'],\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Generar predicciones (logits)\n",
    "print(\"Generando predicciones en el conjunto de prueba...\")\n",
    "logits = model.predict(tf_test).logits\n",
    "predictions = tf.argmax(logits, axis=-1).numpy()\n",
    "true_labels = np.array(test_dataset['label'])\n",
    "\n",
    "# Calcular métricas requeridas\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "# Usamos 'weighted' para el promedio de métricas en clasificación multi-clase\n",
    "recall = recall_score(true_labels, predictions, average='weighted') \n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"\\n--- Resultados Finales de Evaluación ---\")\n",
    "print(f\"Precisión (Accuracy): {accuracy:.4f}\")\n",
    "print(f\"Recall (Sensibilidad): {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tellez-guillent-practica-i",
   "language": "python",
   "name": "tellez-guillent-practica-i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
